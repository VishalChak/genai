{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1ba69a2-7ea1-40fb-b0eb-690b85d40295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mbrazel/open-source-self-hosted-rag-llm-server-with-chromadb-docker-ollama-7e6c6913da7a\n",
    "\n",
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10c238b0-5d9a-4540-a560-47c82da4d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"2404.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "216578bb-41a5-412a-a240-be9dd6499e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.get_or_create_collection(name=\"my_collection\")\n",
    "for doc in docs:\n",
    "    collection.add(\n",
    "        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d05276b9-21d5-4b46-af0a-80ba0efd70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "def Extract_context(query):\n",
    "    chroma_client = chromadb.HttpClient(host='localhost', port=8000,settings=Settings(allow_reset=True))\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(\n",
    "        client=chroma_client,\n",
    "        collection_name=\"my_collection\",\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    docs = db.similarity_search(query)\n",
    "    fullcontent =''\n",
    "    for doc in docs:\n",
    "        fullcontent ='. '.join([fullcontent,doc.page_content])\n",
    "\n",
    "    return fullcontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86081196-a66e-4c63-87ea-49154a4e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_message_rag(content):\n",
    "        return f\"\"\"You are an expert consultant helping executive advisors to get relevant information from internal documents.\n",
    "\n",
    "        Generate your response by following the steps below:\n",
    "        1. Recursively break down the question into smaller questions.\n",
    "        2. For each question/directive:\n",
    "            2a. Select the most relevant information from the context in light of the conversation history.\n",
    "        3. Generate a draft response using selected information.\n",
    "        4. Remove duplicate content from draft response.\n",
    "        5. Generate your final response after adjusting it to increase accuracy and relevance.\n",
    "        6. Do not try to summarise the answers, explain it properly.\n",
    "        6. Only show your final response! \n",
    "        \n",
    "        Constraints:\n",
    "        1. DO NOT PROVIDE ANY EXPLANATION OR DETAILS OR MENTION THAT YOU WERE GIVEN CONTEXT.\n",
    "        2. Don't mention that you are not able to find the answer in the provided context.\n",
    "        3. Don't make up the answers by yourself.\n",
    "        4. Try your best to provide answer from the given context.\n",
    "\n",
    "        CONTENT:\n",
    "        {content}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c662b929-b3e6-47fc-be46-cb6cfa2860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_response_prompt(question):\n",
    "    return f\"\"\"\n",
    "    ==============================================================\n",
    "    Based on the above context, please provide the answer to the following question:\n",
    "    {question}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "487a12a3-41d0-48e9-b9e5-76aa0b605199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(content,question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    stream = client.chat(model='mistral', messages=[\n",
    "    {\"role\": \"system\", \"content\": get_system_message_rag(content)},            \n",
    "    {\"role\": \"user\", \"content\": get_ques_response_prompt(question)}\n",
    "    ],stream=True)\n",
    "    # print(get_system_message_rag(content))\n",
    "    # print(get_ques_response_prompt(question))\n",
    "    print(\"####### THINKING OF ANSWER............ \")\n",
    "    full_answer = ''\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        full_answer =''.join([full_answer,chunk['message']['content']])\n",
    "\n",
    "    return full_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8f53ffe-8452-49e2-bd9f-d4683f8e5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8928\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is Vishal Babu'\n",
    "context = Extract_context(query)\n",
    "print(len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a674db70-6d59-4a41-824a-ebde815bb1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### THINKING OF ANSWER............ \n",
      " The provided text does not contain information about \"Vishal Babu\". It primarily discusses the concepts of multi-head attention and compressive memory in the context of Transformer models. There is no mention or indication that Vishal Babu is associated with these topics or any other specific details about him. The provided text does not contain information about \"Vishal Babu\". It primarily discusses the concepts of multi-head attention and compressive memory in the context of Transformer models. There is no mention or indication that Vishal Babu is associated with these topics or any other specific details about him.\n"
     ]
    }
   ],
   "source": [
    "res = generate_rag_response(context ,query )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe93d77-7e76-408e-a926-5d1ae259d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/query', methods=['POST'])\n",
    "def respond_to_query():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        # Assuming the query is sent as a JSON object with a key named 'query'\n",
    "        query = data.get('query')\n",
    "        # Here you can process the query and generate a response\n",
    "        response = f'This is the response to your query:\\n {get_reponse(query)}'\n",
    "        return response\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290d986-081d-4f9b-be47-b84f7559bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffa6ca-f223-4719-b8dd-596b23d53afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
