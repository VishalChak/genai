{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ba69a2-7ea1-40fb-b0eb-690b85d40295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mbrazel/open-source-self-hosted-rag-llm-server-with-chromadb-docker-ollama-7e6c6913da7a\n",
    "\n",
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c238b0-5d9a-4540-a560-47c82da4d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishal/anaconda3/envs/genai/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/vishal/anaconda3/envs/genai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"2404.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216578bb-41a5-412a-a240-be9dd6499e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishal/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|████████████████████████████████████████████| 79.3M/79.3M [00:35<00:00, 2.35MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.get_or_create_collection(name=\"my_collection\")\n",
    "for doc in docs:\n",
    "    collection.add(\n",
    "        ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05276b9-21d5-4b46-af0a-80ba0efd70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "def Extract_context(query):\n",
    "    chroma_client = chromadb.HttpClient(host='localhost', port=8000,settings=Settings(allow_reset=True))\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(\n",
    "        client=chroma_client,\n",
    "        collection_name=\"my_collection\",\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    docs = db.similarity_search(query)\n",
    "    fullcontent =''\n",
    "    for doc in docs:\n",
    "        fullcontent ='. '.join([fullcontent,doc.page_content])\n",
    "\n",
    "    return fullcontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86081196-a66e-4c63-87ea-49154a4e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_message_rag(content):\n",
    "        return f\"\"\"You are an expert consultant helping executive advisors to get relevant information from internal documents.\n",
    "\n",
    "        Generate your response by following the steps below:\n",
    "        1. Recursively break down the question into smaller questions.\n",
    "        2. For each question/directive:\n",
    "            2a. Select the most relevant information from the context in light of the conversation history.\n",
    "        3. Generate a draft response using selected information.\n",
    "        4. Remove duplicate content from draft response.\n",
    "        5. Generate your final response after adjusting it to increase accuracy and relevance.\n",
    "        6. Do not try to summarise the answers, explain it properly.\n",
    "        6. Only show your final response! \n",
    "        \n",
    "        Constraints:\n",
    "        1. DO NOT PROVIDE ANY EXPLANATION OR DETAILS OR MENTION THAT YOU WERE GIVEN CONTEXT.\n",
    "        2. Don't mention that you are not able to find the answer in the provided context.\n",
    "        3. Don't make up the answers by yourself.\n",
    "        4. Try your best to provide answer from the given context.\n",
    "\n",
    "        CONTENT:\n",
    "        {content}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c662b929-b3e6-47fc-be46-cb6cfa2860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ques_response_prompt(question):\n",
    "    return f\"\"\"\n",
    "    ==============================================================\n",
    "    Based on the above context, please provide the answer to the following question:\n",
    "    {question}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487a12a3-41d0-48e9-b9e5-76aa0b605199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "def generate_rag_response(content,question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    stream = client.chat(model='mistral', messages=[\n",
    "    {\"role\": \"system\", \"content\": get_system_message_rag(content)},            \n",
    "    {\"role\": \"user\", \"content\": get_ques_response_prompt(question)}\n",
    "    ],stream=True)\n",
    "    # print(get_system_message_rag(content))\n",
    "    # print(get_ques_response_prompt(question))\n",
    "    full_answer = ''\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        full_answer =''.join([full_answer,chunk['message']['content']])\n",
    "\n",
    "    return full_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f53ffe-8452-49e2-bd9f-d4683f8e5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9584\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is Vishal Babu'\n",
    "context = Extract_context(query)\n",
    "print(len(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a674db70-6d59-4a41-824a-ebde815bb1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### THINKING OF ANSWER............ \n",
      " The text provided does not contain any information about a person named \"Vishal Babu.\" The text provided does not contain any information about a person named \"Vishal Babu.\"\n"
     ]
    }
   ],
   "source": [
    "res = generate_rag_response(context ,query )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe93d77-7e76-408e-a926-5d1ae259d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/query', methods=['POST'])\n",
    "def respond_to_query():\n",
    "    if request.method == 'POST':\n",
    "        data = request.get_json()\n",
    "        # Assuming the query is sent as a JSON object with a key named 'query'\n",
    "        query = data.get('query')\n",
    "        # Here you can process the query and generate a response\n",
    "        response = f'This is the response to your query:\\n {get_reponse(query)}'\n",
    "        return response\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4290d986-081d-4f9b-be47-b84f7559bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ffa6ca-f223-4719-b8dd-596b23d53afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import uuid\n",
    "import chromadb\n",
    "from ollama import Client\n",
    "from flask import Flask, request\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "def load_documents(data_fol):\n",
    "    for file_name in glob.glob(os.path.join(data_fol, \"*.pdf\")):\n",
    "        loader = PyPDFLoader(file_name)\n",
    "        pages = loader.load_and_split()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        docs = text_splitter.split_documents(pages)\n",
    "        for doc in docs:\n",
    "            collection.add(ids=[str(uuid.uuid1())], metadatas=doc.metadata, documents=doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80865708-f2f7-408c-8030-8ffb21db4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_documents('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c5ea3e3-2bf3-4e1f-9892-0ec3e17ef5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_context(query):\n",
    "    chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(\n",
    "        client=chroma_client,\n",
    "        collection_name=\"my_collection\",\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "    docs = db.similarity_search(query)\n",
    "    fullcontent =''\n",
    "    for doc in docs:\n",
    "        fullcontent ='. '.join([fullcontent,doc.page_content])\n",
    "\n",
    "    return fullcontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a5446c-4f2a-4070-aea8-18aeaec566be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". VISHALBABU\\nDataScientist|MTech(CSE-IS)|NITKSurathkal\\nPune,India411036|+91-8802901311|vishalbabu.in@gmail.com\\nlinkedin.com/in/vishalchak/|github.com/VishalChak|hackerrank.com/profile/VishalChak\\nSUMMARY\\nWithover8.5yearsofexperienceinDataScience&Development,IspecializeinML,DL,NLP,ComputerVision,AI,\\nandsoftwareengineering.Proficientingen_ai,LLM,CNN,RNN,LSTM,GAN,PySpark,Tensorflow,Keras,PyTorch,\\nNLTK,spacy,Docker,Flask,AWS,andmore.Experiencedinobjectdetection,classification,tracking,andvariousother\\ntechnologies.Myskillsenablemetodeliverimpactfulsolutionsacrossdiversedomains.\\nTECHNICALSKILLS\\n\\uf0b7DataScience/MachineLearning/Deep\\nLearning:ModelDevelopment,DataVisualization,\\nSupervised/UnsupervisedLearningAlgos,Deep\\nLearning(ANN,CNN,NLP,tranformers,LLM's),\\nEDA,FeatureEngineering,VectorDB,PEFT,RAG,\\nlora,qlora,PromptEngineeringandPCAetc.\\n\\uf0b7ProgrammingLanguages:Python,Sql,C,\\nNoSQL,Java,PySpark\\n\\uf0b7Cloudtechnologies:AWS(EC2,S3Kinesis,\\nSageMaker,ECR),GCP,Azureetc\\uf0b7PythonPackagesandFrameworks:scikit-\\nlearn,tensorflow,keras,pytorch,numpy,pandas,\\ntransformers,opencv,seaborn,matplotlib,\\nlangchain,rasa,SparkML,Spacy,NTLK,Standford-\\nNLP,Ollama,ChomaDBsqlite3,request,\\nbeautifulsoupandseleniumetc.\\n\\uf0b7MathematicsforML&DL:Algebra,Probability,\\nStatistics,Calculus,Matrices\\n\\uf0b7Miscellaneous:DataStructures,streamlit,Git,\\nDocker,webtechnologies.\\nEXPERIENCE\\n10/2020-CurrentLeadDataScientist\\nInstantSys-Noida,India\\n\\uf0b7AtInstantSystem,IserveasaLeadDataScientist,drivinginnovativeprojectssuchas\\nICNowandCloveDental.ICNowrevolutionizesthesemiconductorindustrywithits\\nAI-poweredapplicanttrackingandcandidaterecommendationsystems.Meanwhile,\\nCloveDentalemploysadvancedcomputervisiontomaintaindentalchaircleanliness\\nandoccupancytracking,fullycompliantwithCOVID-19protocols.\\n08/2019-10/2020Sr.DataScientist\\nBlackstraw.ai-Pune,India\\n\\uf0b7AtBlackstraw,IworkedasaSeniorDataScientistonmanyinterestingprojects,\\nincludingreal-timeobjectdetection,classificationandtracking(ODCT),anomaly\\ndetectioninmanufacturing,autonomousnavigation,andMandarintoEnglish\\nmachinetranslation.\\n09/2018-08/2019DataScientist\\nEquinix-Singapore\\n\\uf0b7AsaDataScientistatEquinix,Ispearheadedend-to-enddatascienceprojects,taking\\nthemfromproofofconcept(POC)tofullprojectdelivery.Myroleinvolvedidentifying\\npromisingmachinelearningusecases,conductingPOCs,transformingsuccessful\\nPOCsintofull-fledgedprojectsorservices,andseamlesslyintegratingthese\\ninnovationsintoexistingproducts.. Preprint. Under review.\\nSegment 1 Segment 2 Segment 3 \\nSegment 1 Segment 2 Segment 3 Transformer block: Infini-T ransformer \\nT ransformer-XL Compressive memory: \\nMemory update: \\nMemory retrieval: \\nEffective context: \\nInput segment: Segment 1 \\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\\n(bottom) discards old contexts since it caches the KV states for the last segment only.\\nof parallel compressive memory per attention layer ( His the number of attention heads) in\\naddition to the dot-product attention.\\n2.1.1 Scaled Dot-product Attention\\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\\nLLMs. The MHA’s strong capability to model context-dependent dynamic computation and\\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\\ngenerative models.\\nA single head in the vanilla MHA computes its attention context Adot∈I RN×dvaluefrom\\nsequence of input segments X∈I RN×dmodel as follows. First, it computes attention query,\\nkey, and value states:\\nK=XW K,V=XW Vand Q=XW Q. (1)\\nHere, WK∈I Rdmodel×dkey,WV∈I Rdmodel×dvalueand WQ∈I Rdmodel×dkeyare trainable projection\\nmatrices. Then, the attention context is calculated as a weighted average of all other values\\nas\\nAdot=softmax\\x12QKT\\n√dmodel\\x13\\nV. (2)\\nFor MHA, we compute Hnumber of attention context vectors for each sequence element\\nin parallel, concatenate them along the second dimension and then finally project the\\nconcatenated vector to the model space to obtain attention the output.\\n2.1.2 Compressive Memory\\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\\nreuse the query, key and value states ( Q,Kand V) from the dot-product attention compu-\\ntation. The state sharing and reusing between the dot-product attention and compressive\\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up\\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\\nstore bindings of key and value states in the compressive memory and retrieve by using the\\nquery vectors.\\n3. 07/2016-09/2018MachineLearningEngineer\\nLarsen&ToubroInfotech(LTI)-Pune,India\\n\\uf0b7AsaMachineLearningEngineeratLTI,IledR&Donnewtechnologiesandmanaged\\nend-to-endprojectdelivery.Itransformedrawdataintoinsights,understoodclient\\nproblemstatements,gatheredrequirements,designedpredictivemodels,anddelivered\\nsolutionsasproductsorservices.Myroleencompasseddrivinginnovationand\\nensuringseamlessprojectexecution.\\n07/2015-07/2016BackendDeveloper\\nAutoRABITInc.-Hyderabad\\n\\uf0b7AsaBackendDeveloper,IcraftedrobustmodulesandAPIcallsusingJava,JavaScript,\\nandjQuery.Idesignedandimplementedservicesfromscratch,handlingeverything\\nfromresearchanddevelopmenttointegration.Notableachievementsinclude\\ndevelopingEZcheck-in,implementinguser-basedauto-commit,integratingGIT,SVN,\\nTFS,andPerforce,andcreatingaGIT-hostedTFSwrapper.\\nEDUCATIONANDTRAINING\\n07/2015 M.Tech(CSE-IS)\\nNationalInstituteofTechnologyKarnataka(NITK)-Surathkal\\nGrade:7.25\\n07/2011 B.Tech(CSE-IT)\\nUnitedCollegeofEngineering&Research(UCER)-GreaterNoida\\nGrade:69.95\\nPROFESSIONALPROJECTS\\n\\uf0b7Objectdetectionclassificationandtracking(ODCT):Webuiltareal-time\\nObjectDetection,Classification,andTracking(ODCT)systemforBFL.ItusesYOLO\\nforobjectdetectionandDeep-SORTfortracking,trainedonPyTorchwithMarsand\\nNvidiaAIcitydatasets.Thissystemenhancessecuritybyidentifyingthreatsand\\ngeneratingalertsfromcamerafeeds.\\n\\uf0b7Anomalydetectioninmanufacturing:We'vedevelopedarobustanomaly\\ndetectionsystemformanufacturingusingcomputervisionanddeeplearning.Our\\nsolutioncombinesHomomorphicfilteringwithDCGAN,enablingeffectiveanomaly\\ndetectionevenwithlimiteddata.\\n\\uf0b7SemanticEmailRoutingSystem:Wecreatedasystemtoclassifyemailsbasedon\\ncontentandroutethemtotherightrecipients.Handlingmultiplelanguages,our\\npipelineincludedlanguagedetection,machinetranslation,textcleansing,LDA,and\\nclusteringforaccuratecategorizationandrouting.\\n\\uf0b7MEDS(IndiaAutomation):MEDS(IndiaAutomation)byMorningstarstreamlines\\ndatacollectionfromorganizations'annualandquarterlyreportssubmittedtotheStock\\nExchangeinPDFformat.Itautomaticallyextractskeyfinancialsectionsanddata\\npoints,comparingthemwithearlierreporteddatausingmachinelearningmodelslike\\nSVM,randomforest,TPOT,andAutokeras.ThesystemincludesAbbyyfordocument\\nprocessing,Kerasfordeeplearning,andFlaskforwebdevelopment,ensuringefficient\\nandaccuratedataextractionandcomparison.\\n\\uf0b7TravelClaimPrediction:LTICorp'sTravelClaimPredictionprojectusesSparkML\\nandSK-Learntoclassifyemployeetravelclaims.Noveltydetectiontechniqueslike. Preprint. Under review.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv\\npreprint arXiv:1901.02860 , 2019.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast\\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\\nProcessing Systems , 35:16344–16359, 2022.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,\\nNanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.\\narXiv preprint arXiv:2307.02486 , 2023.\\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\\narXiv:2402.10171 , 2024.\\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945 , 2023.\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\\narXiv:1410.5401 , 2014.\\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-\\nating the science of language models. arXiv preprint arXiv:2402.00838 , 2024.\\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory . Psychology\\npress, 2005.\\nGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In\\nProceedings of the ninth annual conference of the Cognitive Science Society , pp. 177–186, 1987.\\nJohn J Hopfield. Neural networks and physical systems with emergent collective computa-\\ntional abilities. Proceedings of the national academy of sciences , 79(8):2554–2558, 1982.\\nPentti Kanerva. Sparse distributed memory . MIT press, 1988.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran c ¸ois Fleuret. Transformers\\nare rnns: Fast autoregressive transformers with linear attention. In International conference\\non machine learning , pp. 5156–5165. PMLR, 2020.\\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and\\nSiva Reddy. The impact of positional encoding on length generalization in transformers.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\nWojciech Kry ´sci´nski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir\\nRadev. Booksum: A collection of datasets for long-form narrative summarization. arXiv\\npreprint arXiv:2105.08209 , 2021.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv\\npreprint arXiv:1910.13461 , 2019.\\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for\\nnear-infinite context. arXiv preprint arXiv:2310.01889 , 2023.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language models use long contexts. Transactions\\nof the Association for Computational Linguistics , 12:157–173, 2024.\\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\\nneural networks with backpropagation. In International Conference on Machine Learning ,\\npp. 3559–3568. PMLR, 2018.\\n10\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Extract_context(\"how is vishal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec679ac-e1fc-4669-a4b0-93ff8efe9eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
